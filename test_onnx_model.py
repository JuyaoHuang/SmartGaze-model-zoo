"""
æµ‹è¯•ONNXæ¨¡å‹è¾“å‡ºï¼ŒéªŒè¯è½¬æ¢æ˜¯å¦æ­£ç¡®
"""
import cv2
import numpy as np
import onnxruntime as ort
import sys


def test_onnx_model(onnx_path, test_image_path):
    """
    æµ‹è¯•ONNXæ¨¡å‹è¾“å‡º

    Args:
        onnx_path: ONNXæ¨¡å‹è·¯å¾„
        test_image_path: æµ‹è¯•å›¾ç‰‡ï¼ˆ112x112ï¼‰
    """
    print("=" * 60)
    print("ONNX æ¨¡å‹æµ‹è¯•")
    print("=" * 60)

    # 1. åŠ è½½ONNXæ¨¡å‹
    print(f"\nåŠ è½½ONNXæ¨¡å‹: {onnx_path}")
    session = ort.InferenceSession(onnx_path)
    print("âœ“ ONNXæ¨¡å‹åŠ è½½æˆåŠŸ")

    # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
    input_name = session.get_inputs()[0].name
    output_name = session.get_outputs()[0].name
    print(f"è¾“å…¥åç§°: {input_name}")
    print(f"è¾“å‡ºåç§°: {output_name}")

    # 2. å‡†å¤‡è¾“å…¥
    print(f"\nåŠ è½½æµ‹è¯•å›¾ç‰‡: {test_image_path}")
    img_bgr = cv2.imread(test_image_path)
    if img_bgr is None:
        print(f"é”™è¯¯: æ— æ³•è¯»å–å›¾ç‰‡ {test_image_path}")
        sys.exit(1)

    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
    print(f"å›¾ç‰‡å°ºå¯¸: {img_rgb.shape[:2][::-1]}  (W, H)")

    if img_rgb.shape[:2] != (112, 112):
        print(f"è­¦å‘Š: å›¾ç‰‡å°ºå¯¸ä¸æ˜¯ 112x112ï¼Œæ­£åœ¨ resize...")
        img_rgb = cv2.resize(img_rgb, (112, 112))

    # é¢„å¤„ç†ï¼šå½’ä¸€åŒ–åˆ° [-1, 1]
    img_float = img_rgb.astype(np.float32) / 255.0
    img_normalized = (img_float - 0.5) / 0.5

    # è½¬æ¢ä¸º NCHW æ ¼å¼ (1, 3, 112, 112)
    img_nchw = np.transpose(img_normalized, (2, 0, 1))  # (H,W,C) -> (C,H,W)
    img_batch = np.expand_dims(img_nchw, axis=0)  # (C,H,W) -> (1,C,H,W)

    print(f"è¾“å…¥ Tensor å½¢çŠ¶: {img_batch.shape}")
    print(f"è¾“å…¥ Tensor èŒƒå›´: [{img_batch.min():.4f}, {img_batch.max():.4f}]")

    # 3. æ¨ç†
    print("\nå¼€å§‹æ¨ç†...")
    outputs = session.run([output_name], {input_name: img_batch})
    feature = outputs[0][0]  # (1, 512) -> (512,)

    print(f"âœ“ æ¨ç†æˆåŠŸ")
    print(f"è¾“å‡ºå½¢çŠ¶: {feature.shape}")

    # 4. è®¡ç®— Norm
    raw_norm = np.linalg.norm(feature)

    print("\n" + "=" * 60)
    print("ç‰¹å¾å‘é‡ç»Ÿè®¡")
    print("=" * 60)
    print(f"â­ Raw Norm: {raw_norm:.4f}")
    print(f"ç‰¹å¾èŒƒå›´: [{feature.min():.4f}, {feature.max():.4f}]")
    print(f"ç‰¹å¾å‡å€¼: {feature.mean():.4f}")
    print(f"ç‰¹å¾æ ‡å‡†å·®: {feature.std():.4f}")
    print(f"å‰20ä¸ªç‰¹å¾å€¼: {feature[:20]}")

    # 5. è¯Šæ–­ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ” è¯Šæ–­ç»“æœ")
    print("=" * 60)

    if abs(raw_norm - 1.0) < 0.1:
        print(f"âœ… æ­£å¸¸: Raw Norm â‰ˆ 1.0 (å®é™…å€¼={raw_norm:.4f})")
        print("   -> ONNXæ¨¡å‹åŒ…å«äº†L2å½’ä¸€åŒ–å±‚")
        print("   -> è¾“å‡ºæ˜¯å½’ä¸€åŒ–åçš„ç‰¹å¾ï¼ˆè¿™æ˜¯é¢„æœŸçš„ï¼‰")
        print("   -> ä¸PyTorchæ¨¡å‹è¡Œä¸ºä¸€è‡´")
    else:
        print(f"âš ï¸  å¼‚å¸¸: Raw Norm = {raw_norm:.4f}")
        print("   -> é¢„æœŸåº”è¯¥çº¦ä¸º1.0")
        print("   -> å¯èƒ½ONNXè½¬æ¢æœ‰é—®é¢˜")

    print("\n" + "=" * 60)
    print("ç»“è®º")
    print("=" * 60)
    if abs(raw_norm - 1.0) < 0.1:
        print("âœ… ONNXæ¨¡å‹æ­£å¸¸ï¼")
        print("å¦‚æœRKNNæ¨ç†ä¸å‡†ç¡®ï¼Œé—®é¢˜å¯èƒ½åœ¨ï¼š")
        print("  1. RKNNè½¬æ¢é…ç½®ï¼ˆå·²é€šè¿‡æ‰‹åŠ¨é¢„å¤„ç†è§£å†³ï¼‰")
        print("  2. RKNNé‡åŒ–ç²¾åº¦æŸå¤±ï¼ˆä½¿ç”¨FP16å¯é¿å…ï¼‰")
    else:
        print("âŒ ONNXè½¬æ¢å¯èƒ½æœ‰é—®é¢˜ï¼")
        print("å»ºè®®ï¼š")
        print("  1. é‡æ–°è½¬æ¢ONNXæ¨¡å‹")
        print("  2. æ£€æŸ¥PyTorchæ¨¡å‹æ˜¯å¦æ­£ç¡®")
    print("=" * 60)

    return raw_norm, feature


if __name__ == '__main__':
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python test_onnx_model.py <model.onnx> <test_image.jpg>")
        print("\nç¤ºä¾‹:")
        print("  python test_onnx_model.py outputs/mobilefacenet.onnx ../project/face_app/imgs/1.jpg")
        sys.exit(1)

    onnx_path = sys.argv[1]
    test_image_path = sys.argv[2]

    raw_norm, feature = test_onnx_model(onnx_path, test_image_path)
